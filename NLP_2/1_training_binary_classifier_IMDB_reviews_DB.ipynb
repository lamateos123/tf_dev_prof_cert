{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559a7a4ac2f041759885923ef352e920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce784a560846699c1182ac3cc2d366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating splits...', max=3, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Generating train examples...', max=1, style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling ~\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Generating test examples...', max=1, style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling ~\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Generating unsupervised examples...', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling ~\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb_reviews downloaded and prepared to ~\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\n",
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset.\n",
      "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='~\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Sentiment classification between positive and negative movie reviews\n",
    "# Use TFDS - tensorflow-datasets\n",
    "# !pip install -q tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the IMDB reviews dataset\n",
    "# with_info=True -> If you want to see the description of the dataset\n",
    "# as_supervised=True -> to load the data as (input, label) pairs\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>, Split('test'): <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>, Split('unsupervised'): <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}\n",
      "\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "\n",
    "# Print the contents of the dataset\n",
    "print(imdb)\n",
    "\n",
    "print()\n",
    "# Take 2 training examples and print its contects\n",
    "for i in imdb['train'].take(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the train and test sets\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "# Init sentences and labels lists\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# Loop over all the training examples and save the sentences and labels\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(s.numpy().decode('utf8'))\n",
    "    training_labels.append(l.numpy())\n",
    "    \n",
    "# Loop over all the testing examples and save the sentences and labels\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(s.numpy().decode('utf8'))\n",
    "    testing_labels.append(l.numpy())\n",
    "    \n",
    "# Convert labels list to numpy array\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Padded Sequences\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "embedding_dim = 16\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Init Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the sentiment Model\n",
    "# Use Embedding to represent each word in the vocabulary with vectors\n",
    "# These vectors have trainable weights so as your NN learns\n",
    "# Words that are most likely to appear in positive reviews will converge towards to similar weigths\n",
    "# After the Embedding layer, flatten its output and feed it into a Dense layer\n",
    "# The output is a single sigmoid neuron for 2 classes, use binary_crossentropy as loss function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training paramenters\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - ETA: 6:04 - loss: 0.6946 - accuracy: 0.46 - ETA: 2s - loss: 0.6927 - accuracy: 0.5312 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 1s - loss: 0.6839 - accuracy: 0.56 - ETA: 1s - loss: 0.6795 - accuracy: 0.57 - ETA: 1s - loss: 0.6718 - accuracy: 0.58 - ETA: 1s - loss: 0.6604 - accuracy: 0.60 - ETA: 1s - loss: 0.6489 - accuracy: 0.61 - ETA: 1s - loss: 0.6380 - accuracy: 0.62 - ETA: 0s - loss: 0.6257 - accuracy: 0.63 - ETA: 0s - loss: 0.6132 - accuracy: 0.64 - ETA: 0s - loss: 0.6015 - accuracy: 0.66 - ETA: 0s - loss: 0.5909 - accuracy: 0.66 - ETA: 0s - loss: 0.5814 - accuracy: 0.67 - ETA: 0s - loss: 0.5709 - accuracy: 0.68 - ETA: 0s - loss: 0.5631 - accuracy: 0.69 - ETA: 0s - loss: 0.5539 - accuracy: 0.69 - ETA: 0s - loss: 0.5461 - accuracy: 0.70 - ETA: 0s - loss: 0.5377 - accuracy: 0.71 - ETA: 0s - loss: 0.5302 - accuracy: 0.71 - ETA: 0s - loss: 0.5249 - accuracy: 0.72 - ETA: 0s - loss: 0.5189 - accuracy: 0.72 - ETA: 0s - loss: 0.5120 - accuracy: 0.73 - ETA: 0s - loss: 0.5068 - accuracy: 0.73 - ETA: 0s - loss: 0.5005 - accuracy: 0.73 - ETA: 0s - loss: 0.4958 - accuracy: 0.74 - ETA: 0s - loss: 0.4910 - accuracy: 0.74 - 3s 3ms/step - loss: 0.4897 - accuracy: 0.7466 - val_loss: 0.3810 - val_accuracy: 0.8290\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - ETA: 3s - loss: 0.2700 - accuracy: 0.90 - ETA: 2s - loss: 0.2701 - accuracy: 0.89 - ETA: 2s - loss: 0.2525 - accuracy: 0.90 - ETA: 2s - loss: 0.2469 - accuracy: 0.90 - ETA: 1s - loss: 0.2520 - accuracy: 0.90 - ETA: 1s - loss: 0.2484 - accuracy: 0.90 - ETA: 1s - loss: 0.2508 - accuracy: 0.90 - ETA: 1s - loss: 0.2457 - accuracy: 0.90 - ETA: 1s - loss: 0.2489 - accuracy: 0.90 - ETA: 1s - loss: 0.2521 - accuracy: 0.90 - ETA: 1s - loss: 0.2510 - accuracy: 0.90 - ETA: 1s - loss: 0.2490 - accuracy: 0.90 - ETA: 1s - loss: 0.2468 - accuracy: 0.90 - ETA: 1s - loss: 0.2454 - accuracy: 0.90 - ETA: 1s - loss: 0.2454 - accuracy: 0.90 - ETA: 0s - loss: 0.2447 - accuracy: 0.90 - ETA: 0s - loss: 0.2445 - accuracy: 0.90 - ETA: 0s - loss: 0.2430 - accuracy: 0.90 - ETA: 0s - loss: 0.2422 - accuracy: 0.90 - ETA: 0s - loss: 0.2408 - accuracy: 0.90 - ETA: 0s - loss: 0.2390 - accuracy: 0.90 - ETA: 0s - loss: 0.2380 - accuracy: 0.90 - ETA: 0s - loss: 0.2383 - accuracy: 0.90 - ETA: 0s - loss: 0.2378 - accuracy: 0.90 - ETA: 0s - loss: 0.2376 - accuracy: 0.90 - ETA: 0s - loss: 0.2373 - accuracy: 0.90 - ETA: 0s - loss: 0.2373 - accuracy: 0.90 - ETA: 0s - loss: 0.2364 - accuracy: 0.90 - ETA: 0s - loss: 0.2349 - accuracy: 0.90 - ETA: 0s - loss: 0.2342 - accuracy: 0.90 - ETA: 0s - loss: 0.2333 - accuracy: 0.91 - ETA: 0s - loss: 0.2340 - accuracy: 0.91 - ETA: 0s - loss: 0.2335 - accuracy: 0.91 - ETA: 0s - loss: 0.2340 - accuracy: 0.91 - 2s 3ms/step - loss: 0.2341 - accuracy: 0.9101 - val_loss: 0.4189 - val_accuracy: 0.8142\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - ETA: 2s - loss: 0.1516 - accuracy: 0.93 - ETA: 2s - loss: 0.0966 - accuracy: 0.97 - ETA: 1s - loss: 0.1001 - accuracy: 0.97 - ETA: 1s - loss: 0.0963 - accuracy: 0.97 - ETA: 1s - loss: 0.0967 - accuracy: 0.97 - ETA: 1s - loss: 0.0940 - accuracy: 0.97 - ETA: 1s - loss: 0.0923 - accuracy: 0.98 - ETA: 1s - loss: 0.0906 - accuracy: 0.98 - ETA: 1s - loss: 0.0896 - accuracy: 0.98 - ETA: 1s - loss: 0.0894 - accuracy: 0.98 - ETA: 1s - loss: 0.0903 - accuracy: 0.98 - ETA: 1s - loss: 0.0918 - accuracy: 0.97 - ETA: 1s - loss: 0.0910 - accuracy: 0.97 - ETA: 1s - loss: 0.0907 - accuracy: 0.97 - ETA: 1s - loss: 0.0916 - accuracy: 0.97 - ETA: 0s - loss: 0.0930 - accuracy: 0.97 - ETA: 0s - loss: 0.0932 - accuracy: 0.97 - ETA: 0s - loss: 0.0922 - accuracy: 0.97 - ETA: 0s - loss: 0.0912 - accuracy: 0.97 - ETA: 0s - loss: 0.0911 - accuracy: 0.97 - ETA: 0s - loss: 0.0916 - accuracy: 0.97 - ETA: 0s - loss: 0.0909 - accuracy: 0.97 - ETA: 0s - loss: 0.0906 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0907 - accuracy: 0.97 - ETA: 0s - loss: 0.0901 - accuracy: 0.97 - ETA: 0s - loss: 0.0898 - accuracy: 0.97 - ETA: 0s - loss: 0.0896 - accuracy: 0.97 - ETA: 0s - loss: 0.0898 - accuracy: 0.97 - ETA: 0s - loss: 0.0890 - accuracy: 0.97 - ETA: 0s - loss: 0.0887 - accuracy: 0.97 - ETA: 0s - loss: 0.0889 - accuracy: 0.97 - ETA: 0s - loss: 0.0887 - accuracy: 0.97 - ETA: 0s - loss: 0.0883 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0877 - accuracy: 0.97 - ETA: 0s - loss: 0.0876 - accuracy: 0.97 - ETA: 0s - loss: 0.0871 - accuracy: 0.97 - ETA: 0s - loss: 0.0873 - accuracy: 0.97 - ETA: 0s - loss: 0.0875 - accuracy: 0.97 - ETA: 0s - loss: 0.0872 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - ETA: 0s - loss: 0.0871 - accuracy: 0.97 - ETA: 0s - loss: 0.0869 - accuracy: 0.97 - ETA: 0s - loss: 0.0869 - accuracy: 0.97 - ETA: 0s - loss: 0.0870 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - ETA: 0s - loss: 0.0875 - accuracy: 0.97 - ETA: 0s - loss: 0.0880 - accuracy: 0.97 - ETA: 0s - loss: 0.0878 - accuracy: 0.97 - 4s 5ms/step - loss: 0.0877 - accuracy: 0.9775 - val_loss: 0.5044 - val_accuracy: 0.8096\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - ETA: 5s - loss: 0.0532 - accuracy: 0.96 - ETA: 3s - loss: 0.0326 - accuracy: 0.99 - ETA: 3s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0239 - accuracy: 0.99 - ETA: 3s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0228 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0233 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0217 - accuracy: 0.99 - ETA: 3s - loss: 0.0225 - accuracy: 0.99 - ETA: 3s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 3s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0243 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0238 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0232 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 2s - loss: 0.0227 - accuracy: 0.99 - ETA: 2s - loss: 0.0225 - accuracy: 0.99 - ETA: 2s - loss: 0.0226 - accuracy: 0.99 - ETA: 2s - loss: 0.0224 - accuracy: 0.99 - ETA: 2s - loss: 0.0224 - accuracy: 0.99 - ETA: 2s - loss: 0.0222 - accuracy: 0.99 - ETA: 2s - loss: 0.0220 - accuracy: 0.99 - ETA: 2s - loss: 0.0219 - accuracy: 0.99 - ETA: 2s - loss: 0.0217 - accuracy: 0.99 - ETA: 2s - loss: 0.0215 - accuracy: 0.99 - ETA: 2s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0213 - accuracy: 0.99 - ETA: 1s - loss: 0.0211 - accuracy: 0.99 - ETA: 1s - loss: 0.0209 - accuracy: 0.99 - ETA: 1s - loss: 0.0209 - accuracy: 0.99 - ETA: 1s - loss: 0.0208 - accuracy: 0.99 - ETA: 1s - loss: 0.0209 - accuracy: 0.99 - ETA: 1s - loss: 0.0207 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0202 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0202 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0202 - accuracy: 0.99 - ETA: 0s - loss: 0.0202 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0202 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0202 - accuracy: 0.99 - ETA: 0s - loss: 0.0204 - accuracy: 0.99 - ETA: 0s - loss: 0.0204 - accuracy: 0.99 - ETA: 0s - loss: 0.0204 - accuracy: 0.99 - ETA: 0s - loss: 0.0207 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - 5s 7ms/step - loss: 0.0212 - accuracy: 0.9972 - val_loss: 0.5944 - val_accuracy: 0.8084\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - ETA: 3s - loss: 0.0069 - accuracy: 1.00 - ETA: 2s - loss: 0.0052 - accuracy: 1.00 - ETA: 2s - loss: 0.0057 - accuracy: 1.00 - ETA: 2s - loss: 0.0058 - accuracy: 1.00 - ETA: 2s - loss: 0.0073 - accuracy: 0.99 - ETA: 2s - loss: 0.0069 - accuracy: 0.99 - ETA: 2s - loss: 0.0066 - accuracy: 0.99 - ETA: 2s - loss: 0.0064 - accuracy: 0.99 - ETA: 2s - loss: 0.0063 - accuracy: 0.99 - ETA: 2s - loss: 0.0062 - accuracy: 0.99 - ETA: 2s - loss: 0.0062 - accuracy: 0.99 - ETA: 2s - loss: 0.0061 - accuracy: 0.99 - ETA: 1s - loss: 0.0060 - accuracy: 0.99 - ETA: 1s - loss: 0.0059 - accuracy: 0.99 - ETA: 1s - loss: 0.0058 - accuracy: 0.99 - ETA: 1s - loss: 0.0059 - accuracy: 0.99 - ETA: 1s - loss: 0.0058 - accuracy: 0.99 - ETA: 1s - loss: 0.0059 - accuracy: 0.99 - ETA: 1s - loss: 0.0058 - accuracy: 0.99 - ETA: 1s - loss: 0.0058 - accuracy: 0.99 - ETA: 1s - loss: 0.0057 - accuracy: 0.99 - ETA: 1s - loss: 0.0057 - accuracy: 0.99 - ETA: 1s - loss: 0.0057 - accuracy: 0.99 - ETA: 1s - loss: 0.0056 - accuracy: 0.99 - ETA: 1s - loss: 0.0055 - accuracy: 0.99 - ETA: 1s - loss: 0.0055 - accuracy: 0.99 - ETA: 1s - loss: 0.0054 - accuracy: 0.99 - ETA: 1s - loss: 0.0054 - accuracy: 0.99 - ETA: 1s - loss: 0.0053 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0052 - accuracy: 0.99 - ETA: 1s - loss: 0.0051 - accuracy: 0.99 - ETA: 1s - loss: 0.0051 - accuracy: 0.99 - ETA: 1s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0051 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0050 - accuracy: 0.99 - ETA: 0s - loss: 0.0049 - accuracy: 0.99 - ETA: 0s - loss: 0.0049 - accuracy: 0.99 - ETA: 0s - loss: 0.0049 - accuracy: 0.99 - ETA: 0s - loss: 0.0048 - accuracy: 0.99 - ETA: 0s - loss: 0.0048 - accuracy: 0.99 - ETA: 0s - loss: 0.0048 - accuracy: 0.99 - ETA: 0s - loss: 0.0048 - accuracy: 0.99 - ETA: 0s - loss: 0.0048 - accuracy: 0.99 - 4s 5ms/step - loss: 0.0048 - accuracy: 0.9998 - val_loss: 0.6725 - val_accuracy: 0.8093\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 4s - loss: 7.2451e-04 - accuracy: 1.00 - ETA: 3s - loss: 0.0017 - accuracy: 1.0000   - ETA: 3s - loss: 0.0017 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0018 - accuracy: 1.00 - ETA: 3s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0018 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 2s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0016 - accuracy: 1.00 - ETA: 1s - loss: 0.0016 - accuracy: 1.00 - ETA: 1s - loss: 0.0016 - accuracy: 1.00 - ETA: 1s - loss: 0.0016 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - 4s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7238 - val_accuracy: 0.8106\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - ETA: 5s - loss: 6.6707e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0010 - accuracy: 1.0000   - ETA: 2s - loss: 0.0011 - accuracy: 1.00 - ETA: 2s - loss: 0.0010 - accuracy: 1.00 - ETA: 2s - loss: 9.9260e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.6508e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.6654e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.5565e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.4488e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.1174e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.0862e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.1085e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.9839e-04 - accuracy: 1.00 - ETA: 1s - loss: 9.0448e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.9004e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.8250e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7725e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7658e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7219e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7753e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7433e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7592e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7426e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7498e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7622e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7690e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7534e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.7398e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.7174e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.7082e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6959e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.7205e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6938e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6960e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6900e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6878e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6899e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6839e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6820e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.6834e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.7907e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.7959e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.8255e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.8091e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.8699e-04 - accuracy: 1.00 - ETA: 2s - loss: 8.8638e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.2410e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.2339e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.2125e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.2116e-04 - accuracy: 1.00 - ETA: 2s - loss: 9.8208e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.7873e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.7631e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.7508e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.6860e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.6708e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.6488e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.6440e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.6563e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.6334e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.5969e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.5706e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.5447e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.4914e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.4351e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.3901e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.3572e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.3208e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.3010e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.2610e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.2322e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.2250e-04 - accuracy: 0.99 - ETA: 2s - loss: 9.2195e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.1861e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.1671e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.1489e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.1378e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.0830e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.0588e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.0282e-04 - accuracy: 0.99 - ETA: 1s - loss: 9.0149e-04 - accuracy: 0.99 - ETA: 1s - loss: 8.9580e-04 - accuracy: 0.99 - ETA: 1s - loss: 8.9211e-04 - accuracy: 0.99 - ETA: 1s - loss: 8.8961e-04 - accuracy: 0.99 - ETA: 1s - loss: 8.8711e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.8393e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.7958e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.7646e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.7463e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.7279e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.6994e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.6843e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.6644e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.6438e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.6108e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5791e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5580e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5455e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5124e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.4747e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.4571e-04 - accuracy: 1.00 - 7s 9ms/step - loss: 8.4482e-04 - accuracy: 1.0000 - val_loss: 0.7741 - val_accuracy: 0.8114\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - ETA: 5s - loss: 7.4795e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.7329e-04 - accuracy: 1.00 - ETA: 3s - loss: 5.0495e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.8969e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.8550e-04 - accuracy: 1.00 - ETA: 3s - loss: 4.9544e-04 - accuracy: 1.00 - ETA: 2s - loss: 5.0951e-04 - accuracy: 1.00 - ETA: 2s - loss: 5.0504e-04 - accuracy: 1.00 - ETA: 2s - loss: 5.0140e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.9771e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.9013e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.9371e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.9045e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.8560e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.8498e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.8521e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.8101e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.7456e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.7498e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.7181e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.6663e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.5959e-04 - accuracy: 1.00 - ETA: 2s - loss: 4.6185e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.6107e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5974e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5847e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5949e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5657e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5647e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5784e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5555e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5536e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5635e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5551e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5423e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5204e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5022e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.5097e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.4921e-04 - accuracy: 1.00 - ETA: 1s - loss: 4.4934e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4616e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4779e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4718e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4798e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4747e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4565e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4459e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4332e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4144e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4226e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4023e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4335e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4225e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4229e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4135e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4066e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4010e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.3736e-04 - accuracy: 1.00 - 4s 6ms/step - loss: 4.3737e-04 - accuracy: 1.0000 - val_loss: 0.8201 - val_accuracy: 0.8118\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - ETA: 2s - loss: 2.0063e-04 - accuracy: 1.00 - ETA: 3s - loss: 3.1454e-04 - accuracy: 1.00 - ETA: 2s - loss: 3.0621e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.9681e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.8699e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.9319e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.8508e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.8084e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.8222e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7746e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7615e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7595e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7208e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7185e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7340e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7645e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7477e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7488e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7214e-04 - accuracy: 1.00 - ETA: 2s - loss: 2.7157e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7480e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7497e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7531e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7648e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7476e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7339e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7282e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7143e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.7045e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6863e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6818e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6792e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6711e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6658e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6542e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6471e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6382e-04 - accuracy: 1.00 - ETA: 1s - loss: 2.6343e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6312e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6449e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6445e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6368e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6294e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6244e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6096e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.6019e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5897e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5777e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5786e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5718e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5709e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5680e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5531e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5429e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5380e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5365e-04 - accuracy: 1.00 - 4s 6ms/step - loss: 2.5375e-04 - accuracy: 1.0000 - val_loss: 0.8630 - val_accuracy: 0.8114\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - ETA: 4s - loss: 1.1661e-04 - accuracy: 1.00 - ETA: 3s - loss: 1.8416e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6176e-04 - accuracy: 1.00 - ETA: 3s - loss: 1.6617e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6231e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6321e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6093e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.5842e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6054e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.5946e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6252e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6366e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6396e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6380e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6461e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6396e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6315e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6265e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6135e-04 - accuracy: 1.00 - ETA: 2s - loss: 1.6152e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6187e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6242e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6127e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6194e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6190e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6102e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6088e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.6051e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5981e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5854e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5794e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5769e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5694e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5735e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5656e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5643e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.5564e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5500e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5460e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5439e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5532e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5481e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5453e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5442e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5431e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5375e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5418e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5406e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5341e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5325e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5291e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5279e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5306e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5288e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.5231e-04 - accuracy: 1.00 - 4s 5ms/step - loss: 1.5185e-04 - accuracy: 1.0000 - val_loss: 0.9058 - val_accuracy: 0.8114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x213859b4d08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Adjust parameter to get 1 and ~83%\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Trina the model\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, \n",
    "          validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Visualize Word Embeddings\n",
    "\n",
    "# Tensorflow Embedding Projector\n",
    "\n",
    "# Get embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two files\n",
    "# vecs.tsv -> contains the vector weights of each word in the vocabulary\n",
    "# meta.tsv -> contains the words in the vocabulary\n",
    "\n",
    "# reverse the word index to quickly lookup a word based on a given number\n",
    "reverse_word_index = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate files with a loop\n",
    "# Loop vocab_size-1, skipping the 0 key because it is just for the padding\n",
    "\n",
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Init loop\n",
    "for word_num in range(1, vocab_size):\n",
    "    # Get the word associated at the current index\n",
    "    word_name = reverse_word_index[word_num]\n",
    "    \n",
    "    # Get the embedding weights associated with the current index\n",
    "    word_embedding = embedding_weights[word_num]\n",
    "    \n",
    "    # Write the word name\n",
    "    out_m.write(word_name + \"\\n\")\n",
    "    \n",
    "    # Write the word embedding\n",
    "    out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "    \n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://projector.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
